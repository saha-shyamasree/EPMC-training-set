<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d2 20140930//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">PeerJ</journal-id><journal-id journal-id-type="iso-abbrev">PeerJ</journal-id><journal-id journal-id-type="pmc">PeerJ</journal-id><journal-id journal-id-type="publisher-id">PeerJ</journal-id><journal-title-group><journal-title>PeerJ</journal-title></journal-title-group><issn pub-type="epub">2167-8359</issn><publisher><publisher-name>PeerJ Inc.</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4359122</article-id><article-id pub-id-type="publisher-id">831</article-id><article-id pub-id-type="doi">10.7717/peerj.831</article-id><article-categories><subj-group subj-group-type="heading"><subject>Anthropology</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Mathematical Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Anatomy and Physiology</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational Science</subject></subj-group></article-categories><title-group><article-title><SecTag type="TITLE"><text><SENT sid="0" pm="."><plain>Subject-specific body segment parameter estimation using 3D photogrammetry with multiple cameras </plain></SENT>
</text></SecTag></article-title></title-group><contrib-group><contrib id="author-1" contrib-type="author" corresp="yes"><name><surname>Peyer</surname><given-names>Kathrin E.</given-names></name><xref ref-type="aff" rid="aff-1"/><email>kathrin.peyer@manchester.ac.uk</email></contrib><contrib id="author-2" contrib-type="author"><name><surname>Morris</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff-1"/></contrib><contrib id="author-3" contrib-type="author"><name><surname>Sellers</surname><given-names>William I.</given-names></name><xref ref-type="aff" rid="aff-1"/></contrib><aff id="aff-1"><institution>Faculty of Life Sciences, University of Manchester</institution>, <addr-line>Manchester</addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group><contrib id="editor-1" contrib-type="editor"><name><surname>Maschner</surname><given-names>Herbert</given-names></name></contrib></contrib-group><pub-date pub-type="epub" date-type="pub" iso-8601-date="2015-03-10"><day>10</day><month>3</month><year iso-8601-date="2015">2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>3</volume><elocation-id>e831</elocation-id><history><date date-type="received" iso-8601-date="2014-11-03"><day>3</day><month>11</month><year iso-8601-date="2014">2014</year></date><date date-type="accepted" iso-8601-date="2015-02-18"><day>18</day><month>2</month><year iso-8601-date="2015">2015</year></date></history><permissions><copyright-statement>Â© 2015 Peyer et al.</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Peyer et al.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ) and either DOI or URL of the article must be cited.</license-p></license></permissions><self-uri xlink:href="https://peerj.com/articles/831"/><abstract><p><SecTag type="ABS"><text><SENT sid="1" pm="."><plain>Inertial properties of body segments, such as mass, centre of mass or moments of inertia, are important parameters when studying movements of the human body. </plain></SENT>
<SENT sid="2" pm="."><plain>However, these quantities are not directly measurable. </plain></SENT>
<SENT sid="3" pm="."><plain>Current approaches include using regression models which have limited accuracy: geometric models with lengthy measuring procedures or acquiring and post-processing MRI scans of participants. </plain></SENT>
<SENT sid="4" pm="."><plain>We propose a geometric methodology based on 3D photogrammetry using multiple cameras to provide subject-specific body segment parameters while minimizing the interaction time with the participants. </plain></SENT>
<SENT sid="5" pm="."><plain>A low-cost body scanner was built using multiple cameras and 3D point cloud data generated using structure from motion photogrammetric reconstruction algorithms. </plain></SENT>
<SENT sid="6" pm="."><plain>The point cloud was manually separated into body segments, and convex hulling applied to each segment to produce the required geometric outlines. </plain></SENT>
<SENT sid="7" pm="."><plain>The accuracy of the method can be adjusted by choosing the number of subdivisions of the body segments. </plain></SENT>
<SENT sid="8" pm="."><plain>The body segment parameters of six participants (four male and two female) are presented using the proposed method. </plain></SENT>
<SENT sid="9" pm="."><plain>The multi-camera photogrammetric approach is expected to be particularly suited for studies including populations for which regression models are not available in literature and where other geometric techniques or MRI scanning are not applicable due to time or ethical constraints. </plain></SENT>
</text></SecTag></p></abstract><SecTag type="KEYWORD"><kwd-group kwd-group-type="author"><kwd>Body segment parameters</kwd><kwd>Photogrammetry</kwd><kwd>Structure from motion</kwd><kwd>Subject-specific estimation</kwd><kwd>Geometric modelling</kwd><kwd>Biomechanics</kwd></kwd-group></SecTag><funding-group><award-group id="fund-1"><funding-source>BBSRC</funding-source><award-id>BB/K006029/1</award-id></award-group><funding-statement>Research was funded by the BBSRC (grant number BB/K006029/1). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group></article-meta></front><body><SecTag type="INTRO"><sec sec-type="intro"><title><text><SENT sid="10" pm="."><plain>Introduction </plain></SENT>
</text></title><p><text><SENT sid="11" pm="."><plain>Inertial body segment parameters (BSP) such as mass, centre of mass (CoM) or moment of inertia are used in motion analysis in research as well as in clinical settings. </plain></SENT>
<SENT sid="12" pm="."><plain>Accurate values are essential for techniques such as inverse dynamic analysis to allow the calculation of joint torques based on measured segmental accelerations (Winter, 1979). </plain></SENT>
<SENT sid="13" pm="."><plain>However, it is not straightforward to measure these quantities from subjects directly. </plain></SENT>
<SENT sid="14" pm="."><plain>One approach is to use mathematical models of the body segments and rely on anthropometric measurements to determine the dimensions of the modelled segments. </plain></SENT>
<SENT sid="15" pm="."><plain>This type of methods requires a multitude of anthropometric measurements of the participants and is limited by the accuracy of the mathematical model of the body segments. </plain></SENT>
<SENT sid="16" pm="."><plain>The first mathematical model suggested by Hanavan in 1964 represented 15 body segments as cylinders and spheres and required 25 anthropometric measurements (Hanavan, 1964). </plain></SENT>
<SENT sid="17" pm="."><plain>More detailed models presented by Hatze or Yeadon required a total of 95 or 242 measurements, respectively, rendering these methods inefficient for studies with a large number of participants because of the time and discomfort experienced by the participant to acquire all the measurements needed (Hatze, 1980; Yeadon, 1990). </plain></SENT>
<SENT sid="18" pm="."><plain>Other types of approaches rely on X-ray or MRI based tomography to extract subject-specific BSP from participants. </plain></SENT>
<SENT sid="19" pm="."><plain>Unlike other methods, CT or MRI scans provide information about internal structures such as tissue composition which should improve the reconstruction accuracy (Martin et al., 1989; Mungiole &amp; Martin, 1990; Pearsall, Reid &amp; Livingston, 1996; Bauer et al., 2007). </plain></SENT>
<SENT sid="20" pm="."><plain>However, these approaches are also difficult to implement in large-scale studies due to cost and ethical constraints. </plain></SENT>
<SENT sid="21" pm="."><plain>Alternatively, it is possible to approximate inertial BSP by adjusting previously reported average values or using regression models that require only a very few subject-specific measurements (commonly subject height and weight). </plain></SENT>
<SENT sid="22" pm="."><plain>Such average values and regression models were derived from cadavers or participants in a number of famous studies, such as the ones by Clauser, Dempster or Zatsiorsky (via de Leva) (Dempster, 1955; Clauser, McConville &amp; Young, 1969; McConville, Clauser &amp; Churchill, 1980; Leva, 1996). </plain></SENT>
<SENT sid="23" pm="."><plain>However, the reliability of such regression models is rather low, and the models are only applicable to a population similar to the one used to derive the regression equations. </plain></SENT>
</text></p><p><text><SENT sid="24" pm="."><plain>Recently, other methods have been explored to obtain volumetric data of body segments that, in combination with body density assumptions, can provide subject-specific inertial BSP. Sheets, Corazza &amp; Andriacchi (2010) used a laser to scan the body surface of participants and morph a generic model, which contained joint location information, to the scanned surface. BonnechÃ¨re et al. (2014) used a Kinect sensor to estimate body segment lengths but not the volumetric data required to estimate inertial properties. Clarkson et al. (2012) evaluated the Kinect sensor as a surface scanner using a mannequin, but found the scanning resolution to be quite low. </plain></SENT>
<SENT sid="25" pm="."><plain>Another approach to gain surface data is to use photogrammetry. Jensen (1978) proposed the use of stereophotogrammetry to estimate BSP parameters. </plain></SENT>
<SENT sid="26" pm="."><plain>In his model, the human body was divided into elliptical disks with a thickness of 20 mm, and the radii of the elliptical disks were estimated using images from the front and side. </plain></SENT>
<SENT sid="27" pm="."><plain>The drawback of this approach lies in the simplifying assumptions of representing body segments as the elliptical disks. </plain></SENT>
<SENT sid="28" pm="."><plain>However, it is possible to reconstruct the surface of a 3D object from multiple uncalibrated 2D images taken from different positions without requiring any assumptions to the geometry of the body. </plain></SENT>
<SENT sid="29" pm="."><plain>This principle is referred to as âstructure from motionâ and was initially used for producing 3D models of static objects and landscapes. </plain></SENT>
<SENT sid="30" pm="."><plain>Perhaps the most striking example to date is the âBuilding Rome in a Dayâ project which used images from the Flikr web site (http://www.flickr.com) to generate a 3D model of the whole city (Agarwal et al., 2009). </plain></SENT>
<SENT sid="31" pm="."><plain>The reconstruction of a 3D surface from multiple cameras is two-stage process. </plain></SENT>
<SENT sid="32" pm="."><plain>In stage one, the position, orientation and the parameters of the camera optics are estimated. </plain></SENT>
<SENT sid="33" pm="."><plain>This is achieved by the bundle adjustment algorithm (Triggs et al., 2000) that minimizes the error between the re-projected feature points using estimated camera pose and parameters with the actual feature points in the images. </plain></SENT>
<SENT sid="34" pm="."><plain>In theory, feature points could be chosen manually but this would be cumbersome and potentially not very accurate. </plain></SENT>
<SENT sid="35" pm="."><plain>Instead, Scale Invariant Feature Transform (SIFT) algorithms are employed which automate this process by identifying possible common points between multiple images (Lowe, 1999). </plain></SENT>
<SENT sid="36" pm="."><plain>Stage two uses the calibrated views to produce a dense point cloud model of the 3D object. </plain></SENT>
<SENT sid="37" pm="."><plain>There are a number of possible approaches to achieve this (for review see Seitz et al., 2006) but probably the most widespread current approach is patch-based multi-view stereo reconstruction (Furukawa &amp; Ponce, 2010). </plain></SENT>
<SENT sid="38" pm="."><plain>This photogrammetric approach has gained wide acceptance for producing 3D models in areas such as archaeology (McCarthy, 2014) and palaeontology (Falkingham, 2012), and is even used for markerless motion capture (Sellers &amp; Hirasaki, 2014). </plain></SENT>
</text></p><p><text><SENT sid="39" pm="."><plain>The aim of this paper is to investigate whether an approach based on structure form motion photogrammetric reconstruction can provide person-specific body segment parameters, and to identify the strength and weaknesses of such an approach with regards to ease of implementation, cost-effectiveness, subject comfort and processing time. </plain></SENT>
<SENT sid="40" pm="."><plain>A low-cost body scanner was built using multiple cameras and the body segment parameters of six participants (four male and two female) are presented using the proposed method. </plain></SENT>
</text></p></sec></SecTag><SecTag type="METHODS"><sec sec-type="methods"><title><text><SENT sid="41" pm="."><plain>Methods </plain></SENT>
</text></title><p><text><SENT sid="42" pm="."><plain>Photogrammetry relies on obtaining multiple photographs taken from different locations. </plain></SENT>
<SENT sid="43" pm="."><plain>These photographs can be taken with any suitable device, and for objects that do not move, the most cost-effective option is to take 50 + photographs with a single camera that is moved around the object. </plain></SENT>
<SENT sid="44" pm="."><plain>This has the additional advantage that a single intrinsic calibration can be used, since the camera optics can be considered identical for multiple images. </plain></SENT>
<SENT sid="45" pm="."><plain>However, for subjects that can move, all the photographs must be taken simultaneously so that the subject is in exactly the same position for all the images. </plain></SENT>
<SENT sid="46" pm="."><plain>Simultaneous photographs can be achieved in several different ways including multiple still cameras with synchronised remote controls, multiple USB web cameras, or multiple networked cameras. </plain></SENT>
<SENT sid="47" pm="."><plain>There is probably little to choose between these methods, but initial experimentation found that network/IP cameras provided a cost-effective solution that scaled well. </plain></SENT>
<SENT sid="48" pm="."><plain>The camera resolution should be as high as reasonably possible, since higher resolution images provide more information for the feature extraction algorithms and higher point density in the eventual reconstruction. </plain></SENT>
<SENT sid="49" pm="."><plain>This means that low-resolution cameras such as low cost web cameras and standard resolution video cameras may not be suitable. </plain></SENT>
</text></p><p><text><SENT sid="50" pm="."><plain>Most applications that employ photogrammetry aim to capture surface data in great detail, with the emphasis on creating almost true-to-live 3D models and thus maximizing the point cloud density. </plain></SENT>
<SENT sid="51" pm="."><plain>Some applications require only the information available from the point cloud directly (such as feature point locations) and do not require a surface mesh. </plain></SENT>
<SENT sid="52" pm="."><plain>In fact, meshing algorithms tend to decrease the accuracy of the model (Falkingham, 2012). </plain></SENT>
<SENT sid="53" pm="."><plain>In applications where the reconstructed object is to be 3D-printed (Garsthagen; Hobson; Straub &amp; Kerlin, 2014) or where volumetric data is required (such as for body segment estimations presented in this paper), a (closed) surface mesh needs to be created from the point cloud. </plain></SENT>
<SENT sid="54" pm="."><plain>A high-resolution mesh is commonly desired in 3D printing (e.g., for aesthetic or functional reasons), which requires a large number of photographs and sophisticated algorithms to convert the point cloud to a mesh. </plain></SENT>
<SENT sid="55" pm="."><plain>In this paper, we propose the use of convex hulling to generate simplified geometric outlines of the body segments. </plain></SENT>
<SENT sid="56" pm="."><plain>Convex hulling is robust to low-density surface point clouds (and even potential gaps in the point cloud) and can thus be implemented with ease and run automatically without requiring user input. </plain></SENT>
<SENT sid="57" pm="."><plain>Furthermore, being able to generate surface meshes from a low-density point cloud lowers the number of cameras required to build the 3D scanner (as opposed to needing a large number of cameras to achieve densely packed point clouds). </plain></SENT>
</text></p><sec><title><text><SENT sid="58" pm="."><plain>3D body scanner design </plain></SENT>
</text></title><p><text><SENT sid="59" pm="."><plain>Photogrammetric reconstruction can work well with as few as 4 cameras (Sellers &amp; Hirasaki, 2014) but more cameras are necessary to provide a relatively gap free reconstruction. </plain></SENT>
<SENT sid="60" pm="."><plain>To estimate the minimal number of cameras necessary to achieve a 360Â° reconstruction, we positioned a single camera on a circle of radius 1.6 m and placed a stationary skeletal dummy as a test subject in the centre. </plain></SENT>
<SENT sid="61" pm="."><plain>Images were taken every 5Â° and the point cloud reconstructions using 72, 36, 24, 18, 12 and 9 images, corresponding to angular resolutions ranging from 5Â° to 40Â°, were compared (see Fig. 1A). </plain></SENT>
<SENT sid="62" pm="."><plain>Acceptable reconstructions for the purpose of this paper, i.e., no loss of body segment features, were found with 18 or more cameras although using larger numbers of cameras certainly improved the point cloud density. </plain></SENT>
<SENT sid="63" pm="."><plain>After initial testing, the setup design was adjusted by increasing the radius of the camera placements (to increase the field of view to accommodate outstretched arms), placing the cameras above head-hight and angling the camera views downwards (as opposed to placing the cameras at the bottom or at hip-height) and using asymmetric patterns on the floor in the shared field of view of all cameras. </plain></SENT>
<SENT sid="64" pm="."><plain>The latter greatly aided the reconstruction reliability as the camera calibration algorithm relies on shared features.11Without the patterns on the floor, the camera calibration relied on shared features found on the subject, whereas the patterned floor provided a large (or even completely sufficient) number of features to run the camera calibration algorithm. The network camera was implemented using Raspberry Pi (RPi) modules, type A, each equipped with an 8GB SD card and a Pi camera (http://www.raspberrypi.org). </plain></SENT>
<SENT sid="65" pm="."><plain>These modules run the Linux operating system (Raspbian) and provide a flexible and cost-effective 5 megapixel network camera platform. </plain></SENT>
<SENT sid="66" pm="."><plain>The 18 RPi modules (each with a camera) were attached to a 4.8 m diameter hexagonal frame elevated to height of 2.3 m by six support poles (see Fig. 1B). </plain></SENT>
<SENT sid="67" pm="."><plain>Each RPi module was provided with a USB WiFi receiver (Dynamode WL-700-RX; Dynamode, Manchester, UK) and power was provided using the standard RPi power adapter plugged into a multi-socket attached to each support pole. </plain></SENT>
<SENT sid="68" pm="."><plain>Four 500 W Halogen floodlights were mounted to provide additional lighting to increase the image quality. </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-1" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-1</object-id><label>Figure 1</label><caption><title><text><SENT sid="69" pm="."><plain>Body scanner design. </plain></SENT>
</text></title><p><text><SENT sid="70" pm="."><plain>(A) Point cloud reconstruction with varying number of cameras. </plain></SENT>
<SENT sid="71" pm="."><plain>(B) Schematic representation of the RPi scanner design. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g001"/></fig></SecTag><p><text><SENT sid="72" pm="."><plain>RPi cameras can record either still images or movie files. </plain></SENT>
<SENT sid="73" pm="."><plain>For this application we needed to trigger all the cameras to record a single image at the same instant. </plain></SENT>
<SENT sid="74" pm="."><plain>This was achieved using the open source âCompound Piâ application (http://compoundpi.readthedocs.org), which uses the UDP broadcast protocol to control multiple cameras synchronously from a single server. </plain></SENT>
<SENT sid="75" pm="."><plain>Once the individual images have been recorded, the application provides an interface to download all the images obtained to the server in a straightforward manner. </plain></SENT>
<SENT sid="76" pm="."><plain>Since UDP broadcast is a one-to-many protocol, all the clients will receive the same network packet at the same time and the timing consistency for the images will be of the order of milliseconds which is adequate for a human subject who is trying to stand still. </plain></SENT>
<SENT sid="77" pm="."><plain>Higher precision synchronisation can be achieved using a separate synchronisation trigger but this was unnecessary in this application. </plain></SENT>
</text></p></sec><sec><title><text><SENT sid="78" pm="."><plain>Data acquisition </plain></SENT>
</text></title><p><text><SENT sid="79" pm="."><plain>Full body scans using the RPi setup were obtained from six voluntary participants. </plain></SENT>
<SENT sid="80" pm="."><plain>Additionally, their body weight and height was measured (Table 1). </plain></SENT>
<SENT sid="81" pm="."><plain>The male visible human was used as an additional data set for validation (The National Library of Medicineâs Visual Human Project (Spitzer et al., 1996)). </plain></SENT>
<SENT sid="82" pm="."><plain>The experimental protocol (reference number 13310) was approved by the University of Manchester ethics panel. </plain></SENT>
<SENT sid="83" pm="."><plain>In accordance with the experimental protocol, written consent was obtained from all participants. </plain></SENT>
</text></p><SecTag type="TABLE"><table-wrap id="table-1" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/table-1</object-id><label>Table 1</label><caption><title><text><SENT sid="84" pm="."><plain>Participant mass and weight. </plain></SENT>
</text></title></caption><alternatives><graphic xlink:href="peerj-03-831-g013"/><table frame="hsides" rules="groups"><colgroup span="1"><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/><col span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"><text><SENT sid="85" pm="."><plain>P1 (m) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="86" pm="."><plain>P2 (m) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="87" pm="."><plain>P3 (m) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="88" pm="."><plain>P4 (m) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="89" pm="."><plain>P5 (f) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="90" pm="."><plain>P6 (f) </plain></SENT>
</text></th><th rowspan="1" colspan="1"><text><SENT sid="91" pm="."><plain>VH (m) </plain></SENT>
</text></th></tr></thead><tbody><tr><td rowspan="1" colspan="1"><text><SENT sid="92" pm="."><plain>Mass (kg) </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="93" pm="."><plain>73.4 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="94" pm="."><plain>77.0 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="95" pm="."><plain>88.2 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="96" pm="."><plain>87.8 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="97" pm="."><plain>65.4 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="98" pm="."><plain>55.2 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="99" pm="."><plain>90.3 </plain></SENT>
</text></td></tr><tr><td rowspan="1" colspan="1"><text><SENT sid="100" pm="."><plain>Height (m) </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="101" pm="."><plain>1.81 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="102" pm="."><plain>1.83 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="103" pm="."><plain>1.85 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="104" pm="."><plain>1.83 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="105" pm="."><plain>1.65 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="106" pm="."><plain>1.58 </plain></SENT>
</text></td><td rowspan="1" colspan="1"><text><SENT sid="107" pm="."><plain>1.80 </plain></SENT>
</text></td></tr></tbody></table></alternatives><table-wrap-foot><fn id="table-1fn"><p><text><SENT sid="108" pm="."><plain>Notes. </plain></SENT>
</text></p></fn><fn id="table-1fn1" fn-type="other"><p><text><SENT sid="109" pm="."><plain>P1âP6Participants (m: male, f: female)VHMale visible human </plain></SENT>
</text></p></fn></table-wrap-foot></table-wrap></SecTag><p><text><SENT sid="110" pm="."><plain>The reconstruction algorithms rely on finding matching points across multiple images so do not work well on images that contain no textural variation. </plain></SENT>
<SENT sid="111" pm="."><plain>We therefore experimented with using different types of clothing in the scanner, such as sports clothing, leisure clothing, and a black motion capture suit equipped with Velcro strips to aid feature detection. </plain></SENT>
<SENT sid="112" pm="."><plain>Clothing was either body-tight or tightened using Velcro strips if they were loose, since loose clothing would lead to an overestimation of the body volume. </plain></SENT>
<SENT sid="113" pm="."><plain>The participants stood in the centre of the RPi setup with their hands lifted above their head (see Fig. 2) and the 18 images were then acquired. </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-2" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-2</object-id><label>Figure 2</label><caption><title><text><SENT sid="114" pm="."><plain>Image processing work flow. </plain></SENT>
</text></title><p><text><SENT sid="115" pm="."><plain>Images from the RPI scanner are converted to 3D point clouds which are then scaled and segmented manually. </plain></SENT>
<SENT sid="116" pm="."><plain>Subsequently, convex hulling is used to produce a surface mesh around each body segment. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g002"/></fig></SecTag></sec><sec><title><text><SENT sid="117" pm="."><plain>Data processing </plain></SENT>
</text></title><p><text><SENT sid="118" pm="."><plain>The 3D point cloud reconstruction was initially done using open source application VisualSFM (http://ccwu.me/vsfm/) which performed adequately, but we then switched to using Agisoft PhotoScan Standard Edition v1.0.4 (http://www.agisoft.com) which proved to be much easier to install and use. </plain></SENT>
<SENT sid="119" pm="."><plain>Agisoft PhotoScan also achieved a better reconstruction quality with fewer holes in the point cloud and smoother surfaces.22This is based on the comparison of the best reconstruction result achieved with each software after testing an extensive, but not complete, combination of reconstruction parameters. The parameters used in the reconstructions are reported in Supplemental Information. </plain></SENT>
<SENT sid="120" pm="."><plain>Agisoft PhotoScan runs identically on Windows, Mac or Linux. </plain></SENT>
<SENT sid="121" pm="."><plain>The full 3D reconstruction with 18 images took an average of 30 min using an 8 core 3 GHz Xeon MacPro with 12GB RAM. </plain></SENT>
<SENT sid="122" pm="."><plain>The actual time taken was variable depending on the image file size and the reconstruction parameters. </plain></SENT>
<SENT sid="123" pm="."><plain>The output of the Agisoft PhotoScan is an unscaled 3D point cloud of the participants and surrounding scenery (see Fig. 2), which requires further post-processing to calculate BSP values. </plain></SENT>
<SENT sid="124" pm="."><plain>First, the point cloud was scaled and oriented using CloudDigitizer (Sellers &amp; Hirasaki, 2014), the oriented point clouds were then divided into anatomical segments using Geomagic (http://geomagic.com), and the convex hulls computed in MatlabÂ® (http://www.mathworks.com, see Supplemental Information). </plain></SENT>
<SENT sid="125" pm="."><plain>The reference points for the body segmentation are listed in Supplemental Information Table S1. </plain></SENT>
<SENT sid="126" pm="."><plain>The body segments were all oriented into the standard anatomical pose before the volume, centre of mass and inertial tensor were calculated based on the hull shape and segment density using a custom function implemented in MatlabÂ® (see Supplemental Information). </plain></SENT>
<SENT sid="127" pm="."><plain>The choice of body density is an interesting issue. </plain></SENT>
<SENT sid="128" pm="."><plain>Different tissues within segments have different densities and tissue composition is moderately variable between individuals. </plain></SENT>
<SENT sid="129" pm="."><plain>Indeed variations in density are commonly used to estimate body fat percentage (Siri, 1961; BroÅ¾ek et al., 1963). </plain></SENT>
<SENT sid="130" pm="."><plain>MRI and CT based techniques can allow individual tissue identification and can compensate for this but surface volumetric techniques need to use an appropriate mean value. </plain></SENT>
<SENT sid="131" pm="."><plain>Segment specific densities are available (e.g., (Winter, 1979)) but the quoted trunk density is after subtraction of the lung volume. </plain></SENT>
<SENT sid="132" pm="."><plain>For a surface scan model, we need to use a lower value trunk density that incorporates the volume taken up by the air within the lungs. </plain></SENT>
<SENT sid="133" pm="."><plain>Therefore, for the purpose of this paper a trunk density value of 940 kg/m3 was chosen, while a uniform density of 1000 kg/m3 was assumed for all other body segments (Weinbach, 1938; Pearsall, Reid &amp; Ross, 1994). </plain></SENT>
<SENT sid="134" pm="."><plain>The body mass calculated from the volume was never exactly the same as the recorded body mass, so the density values were adjusted pro-rata to produce a consistent value for total mass. (1)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{upgreek} \usepackage{mathrsfs} \setlength{\oddsidemargin}{-69pt} \begin{document} }{}\begin{eqnarray*} \displaystyle s=\frac{{m}_{\mathrm{Participant}}}{\sum {m}_{\mathrm{SegmHull,i}}}.&amp;&amp;\displaystyle \end{eqnarray*}\end{document}s=mParticipantâmSegmHull,i. The factor s effectively scales the body densities and is thus also applied the moments and products of inertia obtained from the convex hull segments (see Supplemental Information). </plain></SENT>
</text></p></sec></sec></SecTag><SecTag type="RESULTS"><sec sec-type="results"><title><text><SENT sid="135" pm="."><plain>Results </plain></SENT>
</text></title><p><text><SENT sid="136" pm="."><plain>Six participants were scanned using the RPi photogrammetry setup and their point cloud segmented. </plain></SENT>
<SENT sid="137" pm="."><plain>In order to be able to calculate the inertial properties, the point cloud needs to be converted into a closed surface mesh. </plain></SENT>
<SENT sid="138" pm="."><plain>To calculate the volume of an arbitrary shape defined by a surface mesh, the mesh needs to be well defined, i.e., it should be two-manifold, contain no holes in the mesh, and have coherent face orientations. </plain></SENT>
<SENT sid="139" pm="."><plain>The process of converting a point cloud to a well-defined mesh is known as hulling and there are several possible methods available. </plain></SENT>
<SENT sid="140" pm="."><plain>The simplest is the minimum convex hull where the minimum volume convex shape is derived mathematically from the point cloud (www.qhull.org). </plain></SENT>
<SENT sid="141" pm="."><plain>This approach has the advantage of being extremely quick and easy to perform and it is very tolerant of point clouds that may contain holes where the reconstruction algorithm has partially failed. </plain></SENT>
<SENT sid="142" pm="."><plain>However, it will always overestimate the volume unless the shape is convex. </plain></SENT>
<SENT sid="143" pm="."><plain>There are also a number of concave hulling approaches. </plain></SENT>
<SENT sid="144" pm="."><plain>Some are mathematically defined such as AlphaShapes (Edelsbrunner &amp; MÃ¼cke, 1994) and Ball Pivoting (Bernardini et al., 1999) and require additional parameters defining the maximum level of permitted convexity. </plain></SENT>
<SENT sid="145" pm="."><plain>Others are proprietary and can require considerable manual intervention such as the built in hole-filling algorithms in Geomagic. </plain></SENT>
<SENT sid="146" pm="."><plain>This latter group provides the highest quality reconstructions but at the expense of considerable operator time. </plain></SENT>
<SENT sid="147" pm="."><plain>For this paper, we concentrated on convex hulls under the assumption that the level of concavity in individual body segments was likely to be relatively small. </plain></SENT>
<SENT sid="148" pm="."><plain>The relative segment mass of all participants are reported in Fig. 3 (the segmented convex hulls are shown in Fig. </plain></SENT>
<SENT sid="149" pm="."><plain>S1 in Supplemental Information). Figure 3 also displays average values from literature. </plain></SENT>
<SENT sid="150" pm="."><plain>As the participants were imaged wearing shoes, the foot volume was overestimated significantly. </plain></SENT>
<SENT sid="151" pm="."><plain>It is possible to adjust the value using a foot-specific scaling factor that accounts for this overestimation, although of course if the subsequent use of the BSP parameters is in experiments with participants wearing shoes then the shoe mass becomes an important part of the segment. </plain></SENT>
<SENT sid="152" pm="."><plain>For the purpose of this paper, a scaling factor was derived based on a single participant (P5) by comparing the convex hull volume of the foot imaged in socks versus the convex hull volume wearing shoes, and this factor (of 0.51) applied to all participantsâ inertial values of the feet. </plain></SENT>
<SENT sid="153" pm="."><plain>The moments of inertia are shown in Fig. 4 together with average values from literature. </plain></SENT>
<SENT sid="154" pm="."><plain>Geometric methods also allow us to calculate the products of inertia which are otherwise simply assumed to be zero. </plain></SENT>
<SENT sid="155" pm="."><plain>The average products of inertia are depicted in Fig. 5 (absolute values shown only, signed values reported in Supplemental Information (Tables S2âS4). </plain></SENT>
<SENT sid="156" pm="."><plain>Some segments, e.g., the thigh or trunk, have products of inertia that are of a similar order of magnitude as their moments of inertia, which is indicative of a noticeable difference between the inertial principal axes and the anatomical principal axes. </plain></SENT>
<SENT sid="157" pm="."><plain>However, the majority of the products of inertia are significantly smaller than the moments of inertia (of the same segment) by one to two orders of magnitude. Figure 6 contains the relative centre of mass in the longitudinal segment direction, i.e., along the z-axis with the exception of the foot whose longitudinal axis corresponds to the x-axis (see Fig. 2). Figure 7 shows the shift of CoM from the longitudinal axis in the transverse plane (xây plane). </plain></SENT>
<SENT sid="158" pm="."><plain>The CoM values in literature assume a zero shift from the principal anatomical (longitudinal) axis. </plain></SENT>
<SENT sid="159" pm="."><plain>The shift values we found with our geometric method are generally unequal to zero, but they have be to viewed with caution as the placement of the reference anatomical axis itself has uncertainties associated with it. </plain></SENT>
<SENT sid="160" pm="."><plain>The numerical values presented in Figs. </plain></SENT>
<SENT sid="161" pm="."><plain>3â7 and the segment lengths are reported in Supplemental Information (Tables S2âS13). </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-3" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-3</object-id><label>Figure 3</label><caption><title><text><SENT sid="162" pm="."><plain>Segment mass (as % of body mass). </plain></SENT>
</text></title><p><text><SENT sid="163" pm="."><plain>P, Average value of all six participants (error bars show standard deviation). </plain></SENT>
<SENT sid="164" pm="."><plain>Foot mass adjusted by a factor of 0.51 to compensate for volume overestimation due to wearing shoes. </plain></SENT>
<SENT sid="165" pm="."><plain>Z(m), Male average values reported by Zatsiorsky; Z(f), Female average values reported by Zatsiorsky (Leva, 1996; Zatsiorsky, 2002); D(m), Male average values by Dempster (via Zatsiorsky) (Dempster, 1955; Zatsiorsky, 2002). </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g003"/></fig></SecTag><SecTag type="FIG"><fig id="fig-4" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-4</object-id><label>Figure 4</label><caption><title><text><SENT sid="166" pm="."><plain>Moment of inertia in (104 kg m2) </plain></SENT>
</text></title><p><text><SENT sid="167" pm="."><plain>P, Average value of all six participants (error bars show standard deviation). </plain></SENT>
<SENT sid="168" pm="."><plain>Foot moment of inertia adjusted by a factor of 0.51 to compensate for volume overestimation due to wearing shoes. </plain></SENT>
<SENT sid="169" pm="."><plain>Z(m), Male average values reported by Zatsiorsky; Z(f), Female average values reported by Zatsiorsky (Leva, 1996; Zatsiorsky, 2002). </plain></SENT>
<SENT sid="170" pm="."><plain>The definition of the coordinate system is shown in Fig. 2. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g004"/></fig></SecTag><SecTag type="FIG"><fig id="fig-5" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-5</object-id><label>Figure 5</label><caption><title><text><SENT sid="171" pm="."><plain>Absolute values of products of inertia in (104 kg m2). </plain></SENT>
</text></title><p><text><SENT sid="172" pm="."><plain>The absolute values of Ixy, Ixz and Iyz are shown together with a positive error bar (negative error bar is symmetrical) equal to one standard deviation. </plain></SENT>
<SENT sid="173" pm="."><plain>The signed values are reported in Supplemental Information (Tables S2âS4). </plain></SENT>
<SENT sid="174" pm="."><plain>The Ixy value of the hand is smaller than 103 kg m2 and is not displayed. </plain></SENT>
<SENT sid="175" pm="."><plain>Foot products of inertia adjusted by a factor of 0.51 to compensate for volume overestimation due to wearing shoes. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g005"/></fig></SecTag><SecTag type="FIG"><fig id="fig-6" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-6</object-id><label>Figure 6</label><caption><title><text><SENT sid="176" pm="."><plain>Centre of mass along the longitudinal axis. </plain></SENT>
</text></title><p><text><SENT sid="177" pm="."><plain>P, Average value of all six participants (error bars show standard deviation). </plain></SENT>
<SENT sid="178" pm="."><plain>Z(m, male; f, female): Average values by Zatsiorsky, adjusted by de Leva. </plain></SENT>
<SENT sid="179" pm="."><plain>The CoM is given as % of the segment length. </plain></SENT>
<SENT sid="180" pm="."><plain>The definition of the segments and reference points are given in Supplemental Information Table S1 - Exceptions: * Foot of participants: Heel and toe end point of participantâs shoes instead of foot. ** Forearm and Upper Arm of Z: Elbow reference point is the elbow joint centre instead of the Olecranon (Leva, 1996; Zatsiorsky, 2002). </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g006"/></fig></SecTag><SecTag type="FIG"><fig id="fig-7" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-7</object-id><label>Figure 7</label><caption><title><text><SENT sid="181" pm="."><plain>CoM shift from the anatomical longitudinal axis in the transverse (xây) plane. </plain></SENT>
</text></title><p><text><SENT sid="182" pm="."><plain>Average values of all six participants are shown (error bars show standard deviation). </plain></SENT>
<SENT sid="183" pm="."><plain>Due to mirror-symmetry, the y-values of the segments on the left- and right-hand side have opposite signs. </plain></SENT>
<SENT sid="184" pm="."><plain>To calculate the average, the sign of the segments on the left-hand side was inverted. </plain></SENT>
<SENT sid="185" pm="."><plain>The CoM is given as % of the segment length. </plain></SENT>
<SENT sid="186" pm="."><plain>The data of the foot is not included due to the participants wearing shoes. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g007"/></fig></SecTag><p><text><SENT sid="187" pm="."><plain>To estimate the effect of the convex hull approximation on the mass estimation versus the original body segment shape, the volumes of a high resolution 3D body scan and of their convex hull approximation were calculated and compared. </plain></SENT>
<SENT sid="188" pm="."><plain>A detailed surface mesh was obtained from the National Library of Medicineâs Visible Human Project (Spitzer et al., 1996) by isosurfacing the optical slices using the VTK toolkit (http://www.vtk.org) and cleaning up the resultant mesh using Geomagic. </plain></SENT>
<SENT sid="189" pm="."><plain>The surface mesh of the 3D body scan was separated into body segments and the volume calculated following the same methodology as used for the point cloud data. </plain></SENT>
<SENT sid="190" pm="."><plain>A convex hull was applied to each body segment and the volume calculated again (see Fig. 8). </plain></SENT>
<SENT sid="191" pm="."><plain>The volume overestimations for each body segment (averaged between left and right) are shown Fig. 9 (column CH). </plain></SENT>
<SENT sid="192" pm="."><plain>Several body segments showed a large relative volume overestimation (using 10% error as a cutoff, although the choice would depend on the required accuracy): foot (26%), shank (31%), hand (47%) and forearm (16%). </plain></SENT>
<SENT sid="193" pm="."><plain>This is due to the relatively strong curvatures in these segments. </plain></SENT>
<SENT sid="194" pm="."><plain>To minimize the effect, these body segments were subdivided (see Fig. 10) and the convex hulls recalculated. </plain></SENT>
<SENT sid="195" pm="."><plain>The results of the divided segments are also shown in Fig. 9 (column CHD), and the decrease in volume overestimation is apparent. </plain></SENT>
<SENT sid="196" pm="."><plain>The volume overestimation of the subdivided foot (11%), shank (11%) and forearm (5%) are at a similar level to the other body segments and would probably be acceptable in many cases. </plain></SENT>
<SENT sid="197" pm="."><plain>The hands show the largest relative mass overestimation still (25%), which is due to its curved position and slightly open fingers. </plain></SENT>
<SENT sid="198" pm="."><plain>The convex hull error of the hand is, however, expected to be significantly smaller if the hand is imaged while being held in a straight position with no gaps between the digits. </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-8" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-8</object-id><label>Figure 8</label><caption><title><text><SENT sid="199" pm="."><plain>Visible human surface mesh. </plain></SENT>
</text></title><p><text><SENT sid="200" pm="."><plain>(A) High-resolution surface mesh. </plain></SENT>
<SENT sid="201" pm="."><plain>(B) Convex hull mesh. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g008"/></fig></SecTag><SecTag type="FIG"><fig id="fig-9" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-9</object-id><label>Figure 9</label><caption><title><text><SENT sid="202" pm="."><plain>Segment volume overestimation of the hulled mesh versus the high-resolution surface mesh of the visible human. </plain></SENT>
</text></title><p><text><SENT sid="203" pm="."><plain>Data shown as the relative difference of the hull with respect to the original mesh. </plain></SENT>
<SENT sid="204" pm="."><plain>CH, Convex hull of body segment; CHD, Convex hull of divided body segments (only segments indicated with an * were subdivided, see Fig. 10). </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g009"/></fig></SecTag><SecTag type="FIG"><fig id="fig-10" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-10</object-id><label>Figure 10</label><caption><title><text><SENT sid="205" pm="."><plain>Subdivision of the body segments with large curvature. </plain></SENT>
</text></title><p><text><SENT sid="206" pm="."><plain>The first row (S) shows the high-resolution surface mesh, the second row (CH) the convex hull of the whole body segment, and the bottom row (CHD) the convex hulls of the subdivided body segments. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g010"/></fig></SecTag><p><text><SENT sid="207" pm="."><plain>Figure 11 contains the relative mass estimations of the original surface mesh, the convex hulls with and without subdivision, and the average and regression model values found in literature. </plain></SENT>
<SENT sid="208" pm="."><plain>With a BMI value of almost 28, the male visible human is not well represented by the average or regression model values found in literature, where the majority of the studies involve relatively athletic people (BMI average of around 24) or obese individuals (BMI over 30). </plain></SENT>
<SENT sid="209" pm="."><plain>The convex hulls of the subdivided segments (CHD in Fig. 11) give the closest approximation to the original mesh and, with the exception of the hands, are within a relative error of less than 5%. </plain></SENT>
<SENT sid="210" pm="."><plain>The relative error of the convex hull of the whole segments (CH in Fig. 11) is larger but closer to the original mesh than average and regression values given in literature. </plain></SENT>
<SENT sid="211" pm="."><plain>The moments of inertia are overestimated as well as they are a product of the mass of the segment. </plain></SENT>
<SENT sid="212" pm="."><plain>Their overestimation follows the same trend as the mass overestimation, i.e., the largest overestimation occurs for the hands, followed by the shanks and feet (see Fig. </plain></SENT>
<SENT sid="213" pm="."><plain>S2 in Supplemental Information), and the subdivided segments produce more accurate values with an average relative error below 10%. </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-11" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-11</object-id><label>Figure 11</label><caption><title><text><SENT sid="214" pm="."><plain>Male visible human segment mass (as % of body mass) of the high-resolution mesh, convex hull, regression model and average values. </plain></SENT>
</text></title><p><text><SENT sid="215" pm="."><plain>S, High-resolution surface mesh; CH, Convex Hull of whole body segments; CHD, Convex Hull with subdivided body segments (only segments indicated with an * were subdivided as shown in Fig. 10); ZR, Values predicted using Zatsiosrkyâs linear regression model (using weight and height); Z, Male average values reported by Zatsiorsky; D, Male average values reported by Dempster (Dempster, 1955; Leva, 1996; Zatsiorsky, 2002). </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g011"/></fig></SecTag></sec></SecTag><SecTag type="DISCUSS"><sec sec-type="discussion"><title><text><SENT sid="216" pm="."><plain>Discussion </plain></SENT>
</text></title><p><text><SENT sid="217" pm="."><plain>We can see from the results that the proposed methodology produces values that are very similar to those derived using regression equations. </plain></SENT>
<SENT sid="218" pm="."><plain>There are no consistent problems, although it is clearly important that the hand is held in a suitable flat position but with fingers adducted so that the hulling can provide an accurate volume estimation. </plain></SENT>
<SENT sid="219" pm="."><plain>We would expect that the photogrammetric process will work as well as any of the published geometrical approaches (Hanavan, 1964; Hatze, 1980) since it is simply an automated process for achieving the same outcome. </plain></SENT>
<SENT sid="220" pm="."><plain>The procedure is currently moderately time consuming in total, but the interaction time with the participant is extremely short and involves no contact, which can be very beneficial for certain experimental protocols or with specific vulnerable participants. </plain></SENT>
<SENT sid="221" pm="."><plain>Since most of the time is spent post-processing the data, we expect that this post-processing could be streamlined considerably by writing dedicated software rather than the current requirement of passing the data through multiple software packages. </plain></SENT>
</text></p><p><text><SENT sid="222" pm="."><plain>In general, regression equations work well for applicable populations and are probably more suitable if body mass distribution is not a major focal point of the research, particularly given that in some cases it can be shown that experimental outcomes are not especially sensitive to the BSP parameters chosen (Yokoi et al., 1998). </plain></SENT>
<SENT sid="223" pm="."><plain>The values generated in our sample are relatively close to those generated by using regression equations but BSP values are highly variable between individuals and current regression equations are only suitable for a very limited range of body shapes. </plain></SENT>
<SENT sid="224" pm="."><plain>This is particularly the case when we are dealing with non-standard groupings such as children, the elderly or people with particularly high or low BMI values. </plain></SENT>
</text></p><p><text><SENT sid="225" pm="."><plain>However there are some specific issues with this technique that could to be improved for a more streamlined and potentially more accurate workflow (see Fig. 12, which summarises the steps involved in estimating body segment parameters using photogrammetry). </plain></SENT>
</text></p><SecTag type="FIG"><fig id="fig-12" orientation="portrait" position="float"><object-id pub-id-type="doi">10.7717/peerj.831/fig-12</object-id><label>Figure 12</label><caption><title><text><SENT sid="226" pm="."><plain>Methodology to estimate subject-specific body segment parameters using photogrammetry. </plain></SENT>
</text></title><p><text><SENT sid="227" pm="."><plain>(A) Photogrammetry; (B) Body segmentation; (C) Segment hulling; (D) Inertial parameter estimation. </plain></SENT>
</text></p></caption><graphic xlink:href="peerj-03-831-g012"/></fig></SecTag><p><text><SENT sid="228" pm="."><plain>Convex hulling of the point cloud is a robust and fast way to produce surface meshes. </plain></SENT>
<SENT sid="229" pm="."><plain>The fact that it systematically overestimates the volume of concave features can be improved by subdividing body segments into smaller parts and the decision then becomes what level of subdivision is appropriate for an acceptable level of accuracy (see Fig. 12C). </plain></SENT>
<SENT sid="230" pm="."><plain>For example, with only one subdivision of the shank and forearm the relative error of their volume overestimation was reduced by a factor of three, and the end result was within 10% of the true value which is probably sufficient in most cases, especially given the level of uncertainty in other parameters such as segment specific density. </plain></SENT>
<SENT sid="231" pm="."><plain>It is important to note that the scaling factor used in our method significantly minimises the segment mass estimation errors introduced by the volume overestimation. </plain></SENT>
<SENT sid="232" pm="."><plain>In fact, if all hulled segment masses (i.e., the product of segment volume and segment-specific density, see Fig. 12D) were overestimated by 10%, the final body segment mass would be calculated correctly due to the scaling factor applied to each segment. </plain></SENT>
<SENT sid="233" pm="."><plain>Therefore, using a pro-rata scaling factor performs best when the relative errors of the volume estimation of each segment are within a small range of each other. </plain></SENT>
</text></p><p><text><SENT sid="234" pm="."><plain>The adoption of one of the concave hulling techniques is likely to lead to a similar level of improvement again with a minimum (but not zero) level of additional work. </plain></SENT>
<SENT sid="235" pm="."><plain>The level of subdivision required not only depends on the body segment, but also the population studied so it may well be appropriate that the segmentation level is adjusted according to the type of study and its sensitivity to inaccuracies in the BSP (i.e., multiple segment subdivisions increase accuracy of volume estimation). </plain></SENT>
<SENT sid="236" pm="."><plain>In this work, a uniform scaling factor and constant body density (apart from the trunk) was assumed. </plain></SENT>
<SENT sid="237" pm="."><plain>It is well known that the density varies among body segments as well as among populations due to different percentages of fat and muscle tissue (Drillis, Contini &amp; Bluestein, 1964; Durnin &amp; Womersley, 1974; Zatsiorsky, 2002). </plain></SENT>
<SENT sid="238" pm="."><plain>Thus, using segment and population specific densities (and scaling factors) may improve the accuracy of the presented methodology if such values are available or derived. </plain></SENT>
<SENT sid="239" pm="."><plain>Similarly, important contributions to segmental mass distribution such as the presence of the lungs within the torso can be explicitly modelled, which may lead to small but important shifts in the centre of mass (Bates et al., 2010). </plain></SENT>
</text></p><p><text><SENT sid="240" pm="."><plain>In terms of technology, the current arrangement of using 18 Raspberry Pi cameras is reasonably straightforward and relatively inexpensive. </plain></SENT>
<SENT sid="241" pm="."><plain>It requires no calibration before use, and the process of moving the subject into the target area is extremely quick. </plain></SENT>
<SENT sid="242" pm="."><plain>However, it does take up a great deal of room in the laboratory, and the current software is reliant on clothing contrast for the reconstructions, which limits the flexibility of the technique. </plain></SENT>
<SENT sid="243" pm="."><plain>This could be improved by projecting a structured light pattern onto the subject so that areas with minimal contrast can be reconstructed accurately (Casey, Hassebrook &amp; Lau, 2008). </plain></SENT>
<SENT sid="244" pm="."><plain>Our results show that 18 cameras is currently the minimum needed for full body reconstruction, and a system with 36 or more cameras would produce better point cloud reconstruction results by minimizing areas of potential occlusions (such as between the legs or between the arms and trunk) and increasing the point cloud density. </plain></SENT>
<SENT sid="245" pm="."><plain>To what degree a more densely packed point cloud would significantly improve the accuracy of the estimated inertial parameters based on convex hulls would be an interesting aspect to investigate further. </plain></SENT>
<SENT sid="246" pm="."><plain>We would expect a denser point cloud to facilitate the use of more complex meshing methods instead of convex hulling. </plain></SENT>
</text></p><p><text><SENT sid="247" pm="."><plain>One future use of this technology is clearly the use of such systems and algorithms for complete motion capture (Sellers &amp; Hirasaki, 2014). </plain></SENT>
<SENT sid="248" pm="."><plain>The limitation currently is that these cameras would need to be closely synchronised, and whilst the proposed system is adequate for producing a single still image, it is currently not able to adequately synchronise video. </plain></SENT>
<SENT sid="249" pm="."><plain>In addition, the video resolution is much lower and this makes the reconstruction more difficult. </plain></SENT>
<SENT sid="250" pm="."><plain>However, we predict that markerless, multiple video camera structure from motion systems will become a much more common mainstream tool for experimental motion capture in the near future. </plain></SENT>
<SENT sid="251" pm="."><plain>Ideally, we could imagine that such a system would both do the motion capture and also the body segment parameter reconstruction, since much of the computational technology would be shared. </plain></SENT>
</text></p></sec></SecTag><SecTag type="CONCL"><sec><title><text><SENT sid="252" pm="."><plain>Conclusion </plain></SENT>
</text></title><p><text><SENT sid="253" pm="."><plain>A methodology based on structure form motion photogrammetric reconstruction has been presented that provides subject-specific body segment parameters. </plain></SENT>
<SENT sid="254" pm="."><plain>The method relies on the surface depth information extracted from multiple photographs of a participant, taken simultaneously from multiple different view points. </plain></SENT>
<SENT sid="255" pm="."><plain>The brief interaction time with the participants (taking all required photos simultaneously, and measuring the height and weight only) makes this a promising method in studies with vulnerable subjects or where cost or ethical constraints do not allow the use of other imaging methods such as CT or MRI scans. </plain></SENT>
<SENT sid="256" pm="."><plain>Unlike regression models that are valid only for a small population sample, we expect the proposed methodology to be able to perform equally well for a wide range of population samples. </plain></SENT>
<SENT sid="257" pm="."><plain>The post-processing time is lengthy compared to using regression models or average values from literature but not compared to processing MRI or CT data. </plain></SENT>
<SENT sid="258" pm="."><plain>The 3D scanner presented in this paper was able to produce a sufficient 3D data points to estimate body segment volumes with only 18 RPi cameras, which kept the hardware cost to a minimum. </plain></SENT>
<SENT sid="259" pm="."><plain>Depending on the accuracy required for the project, we would expect both more cameras and higher resolution cameras to improve the robustness of the 3D point cloud reconstruction. </plain></SENT>
</text></p><p><text><SENT sid="260" pm="."><plain>While the results presented in this work were derived using commercial software such as AgiSoft, Geomagic and MatlabÂ®, we were able to to achieve similar results using open-source software only (such as VisualFMS (http://ccwu.me/vsfm/) for calculating 3D point clouds and MeshLab (http://meshlab.sourceforge.net/) for point cloud segmentation, hulling and BSP calculation). </plain></SENT>
<SENT sid="261" pm="."><plain>This makes our proposed methodology, in combination with the low hardware costs, particularly promising for small-budget projects. </plain></SENT>
</text></p></sec></SecTag><SecTag type="SUPPL"><sec sec-type="supplementary-material" id="supplemental-information"><title><text><SENT sid="262" pm="."><plain>Supplemental Information </plain></SENT>
</text></title><supplementary-material content-type="local-data" id="supp-1"><object-id pub-id-type="doi">10.7717/peerj.831/supp-1</object-id><label>Supplemental Information</label><caption><title><text><SENT sid="263" pm="."><plain>Supporting Information </plain></SENT>
</text></title></caption><media xlink:href="peerj-03-831-s001.pdf"><caption><p><text><SENT sid="264" pm="."><plain>Click here for additional data file. </plain></SENT>
</text></p></caption></media></supplementary-material></sec></SecTag></body><back><SecTag type="ACK_FUND"><ack><p><text4fund><text><SENT sid="265" pm="."><plain>The authors would like to thank Dave Jones for the development of the Compound Pi programme and his generous help with the network setup of the Raspberry Pi scanner. </plain></SENT>
</text></text4fund></p></ack></SecTag><sec sec-type="additional-information"><title>Additional Information and Declarations</title><SecTag type="COMP_INT"><fn-group content-type="competing-interests"><title>Competing Interests</title><fn id="conflict-1" fn-type="conflict"><p><text><SENT sid="266" pm="."><plain>The authors declare there are no competing interests. </plain></SENT>
</text></p></fn></fn-group></SecTag><fn-group content-type="author-contributions"><title>Author Contributions</title><fn id="contribution-1" fn-type="con"><p><text><SENT sid="267" pm="."><plain>Kathrin E. </plain></SENT>
<SENT sid="268" pm="."><plain>Peyer conceived and designed the experiments, performed the experiments, analyzed the data, wrote the paper, prepared figures and/or tables, reviewed drafts of the paper. </plain></SENT>
</text></p></fn><fn id="contribution-2" fn-type="con"><p><text><SENT sid="269" pm="."><plain>Mark Morris conceived and designed the experiments, performed the experiments, analyzed the data, reviewed drafts of the paper. </plain></SENT>
</text></p></fn><fn id="contribution-3" fn-type="con"><p><text><SENT sid="270" pm="."><plain>William I. </plain></SENT>
<SENT sid="271" pm="."><plain>Sellers conceived and designed the experiments, wrote the paper, reviewed drafts of the paper. </plain></SENT>
</text></p></fn></fn-group><fn-group content-type="other"><title>Human Ethics</title><fn id="addinfo-1" fn-type="other"><p><text><SENT sid="272" pm="."><plain>The following information was supplied relating to ethical approvals (i.e., approving body and any reference numbers): </plain></SENT>
</text></p><p><text><SENT sid="273" pm="."><plain>The experimental protocol (reference number 13310) was approved by the University of Manchester ethics panel. </plain></SENT>
</text></p></fn></fn-group></sec><SecTag type="REF"><ref-list content-type="authoryear"><title>References</title><ref id="ref-1"><text><SENT sid="274" pm="."><plain>Agarwal et al. (2009)AgarwalSSnavelyNSimonISeitzSMSzeliskiRBuilding Rome in a dayIEEE international conference on computer vision2009IEEEPiscataway7279 </plain></SENT>
</text></ref><ref id="ref-2"><text><SENT sid="275" pm="."><plain>Bates et al. (2010)BatesKTManningPLMargettsLSellersWISensitivity analysis in evolutionary robotic simulations of bipedal dinosaur runningJournal of Vertebrate Paleontology20103045846610.1080/02724630903409329 </plain></SENT>
</text></ref><ref id="ref-3"><text><SENT sid="276" pm="."><plain>Bauer et al. (2007)BauerJJPavolMJSnowCMHayesWCMRI-derived body segment parameters of children differ from age-based estimates derived using photogrammetryJournal of Biomechanics2007402904291010.1016/j.jbiomech.2007.03.00617462656 </plain></SENT>
</text></ref><ref id="ref-4"><text><SENT sid="277" pm="."><plain>Bernardini et al. (1999)BernardiniFMittlemanJRushmeierHSilvaCTaubinGThe ball-pivoting algorithm for surface reconstructionIEEE Transactions on Visualization and Computer Graphics1999534935910.1109/2945.817351 </plain></SENT>
</text></ref><ref id="ref-5"><text><SENT sid="278" pm="."><plain>BonnechÃ¨re et al. (2014)BonnechÃ¨reBJansenBSalviaPBouzahoueneHSholukhaVCornelisJRoozeMVan Sint JanSDetermination of the precision and accuracy of morphological measurements using the KinectTM sensor: comparison with standard stereophotogrammetryErgonomics20145762263110.1080/00140139.2014.88424624646374 </plain></SENT>
</text></ref><ref id="ref-6"><text><SENT sid="279" pm="."><plain>BroÅ¾ek et al. (1963)BroÅ¾ekJGrandeFAndersonJTKeysADensitometric analysis of body composition: revision of some quantitative assumptionsAnnals of the New York Academy of Sciences196311011314010.1111/j.1749-6632.1963.tb17079.x14062375 </plain></SENT>
</text></ref><ref id="ref-7"><text><SENT sid="280" pm="."><plain>Casey, Hassebrook &amp; Lau (2008)CaseyCJHassebrookLGLauDLAsaiKStructured light illumination methods for continuous motion hand and face-computer interactionHuman computer interaction: new developments2008RijekaInTech297308Available at http://www.intechopen.com/books/human_computer_interaction_new_developments/structured_light_illumination_methods_for_continuous_motion_hand_and_face-computer_interaction </plain></SENT>
</text></ref><ref id="ref-8"><text><SENT sid="281" pm="."><plain>Clarkson et al. (2012)ClarksonSChoppinSHartJHellerBWheatJCalculating body segment inertia parameters from a single rapid scan using the microsoft kinectProceedings of the 3rd international conference on 3D body scanning technologies2012LuganoSwitzerland153163 </plain></SENT>
</text></ref><ref id="ref-9"><text><SENT sid="282" pm="."><plain>Clauser, McConville &amp; Young (1969)ClauserCECMcConvilleJJTYoungJWJWeight, volume, and center of mass of segments of the human body1969Wright-Patterson AFB: Air Force Command SystemsAvailable at http://www.dtic.mil/dtic/tr/fulltext/u2/710622.pdf </plain></SENT>
</text></ref><ref id="ref-10"><text><SENT sid="283" pm="."><plain>Dempster (1955)DempsterWSpace requirements of the seated operator: geometrical, kinematic, and mechanical aspects of the body, with special reference to the limbs1955Wright-Patterson AFB: Air Force Command Systems </plain></SENT>
</text></ref><ref id="ref-11"><text><SENT sid="284" pm="."><plain>Drillis, Contini &amp; Bluestein (1964)DrillisRContiniRBluesteinMBody segment parametersArtificial Limbs19648446614208177 </plain></SENT>
</text></ref><ref id="ref-12"><text><SENT sid="285" pm="."><plain>Durnin &amp; Womersley (1974)DurninJWomersleyJBody fat assessed from total body density and its estimation from skinfold thickness: measurements on 481 men and woman aged from 16 to 72 yearsBritish Journal of Nutrition197432779710.1079/BJN197400604843734 </plain></SENT>
</text></ref><ref id="ref-13"><text><SENT sid="286" pm="."><plain>Edelsbrunner &amp; MÃ¼cke (1994)EdelsbrunnerHMÃ¼ckeEPThree-dimensional alpha shapesACM Transactions on Graphics199413437210.1145/174462.156635 </plain></SENT>
</text></ref><ref id="ref-14"><text><SENT sid="287" pm="."><plain>Falkingham (2012)FalkinghamPLAcquisition of high resolution three-dimensional models using free, open-source, photogrammetric softwarePalaeontologia Electronica1T20121515 </plain></SENT>
</text></ref><ref id="ref-15"><text><SENT sid="288" pm="."><plain>Furukawa &amp; Ponce (2010)FurukawaYPonceJAccurate, dense, and robust multiview stereopsisIEEE Transactions on Pattern Analysis and Machine Intelligence2010321362137610.1109/TPAMI.2009.16120558871 </plain></SENT>
</text></ref><ref id="ref-16"><text><SENT sid="289" pm="."><plain>Garsthagen (a)GarsthagenRPi3dscan. Available at www.pi3dscan.com (accessed 2 March 2015) </plain></SENT>
</text></ref><ref id="ref-17"><text><SENT sid="290" pm="."><plain>Hanavan (1964)HanavanEPA mathematical model of the human body1964Wright-Patterson AFB: Air Force Aerospace Medical Research Lab </plain></SENT>
</text></ref><ref id="ref-18"><text><SENT sid="291" pm="."><plain>Hatze (1980)HatzeHA mathematical model for the computational determination of parameter values of anthropomorphic segmentsJournal of Biomechanics19801383384310.1016/0021-9290(80)90171-27462257 </plain></SENT>
</text></ref><ref id="ref-19"><text><SENT sid="292" pm="."><plain>Hobson (a)HobsonJAn affordable full body studio grade 3D scanner. Available at http://hackaday.com/2014/03/07/an-affordable-full-body-studio-grade-3d-scanner/ (accessed 2 March 2015) </plain></SENT>
</text></ref><ref id="ref-20"><text><SENT sid="293" pm="."><plain>Jensen (1978)JensenREstimation of the biomechanical properties of three body types using a photogrammetric methodJournal of Biomechanics19781134935810.1016/0021-9290(78)90069-6711783 </plain></SENT>
</text></ref><ref id="ref-21"><text><SENT sid="294" pm="."><plain>Leva (1996)De LevaPAdjustment to Zatsiorsky-Seluyanovâs segment inertia parametersJournal of Biomechanics1996291223123010.1016/0021-9290(95)00178-68872282 </plain></SENT>
</text></ref><ref id="ref-22"><text><SENT sid="295" pm="."><plain>Lowe (1999)LoweDGObject recognition from local scale-invariant featuresProceedings of the seventh IEEE international conference on computer vision, vol. </plain></SENT>
<SENT sid="296" pm="."><plain>21999IEEEPiscataway11501157 </plain></SENT>
</text></ref><ref id="ref-23"><text><SENT sid="297" pm="."><plain>Martin et al. (1989)MartinPMungioleMMarzkeMLonghillJThe use of magnetic resonance imaging for measuring segment inertial propertiesJournal of Biomechanics19892236737610.1016/0021-9290(89)90051-12745471 </plain></SENT>
</text></ref><ref id="ref-24"><text><SENT sid="298" pm="."><plain>McCarthy (2014)McCarthyJMulti-image photogrammetry as a practical tool for cultural heritage survey and community engagementJournal of Archaeological Science20144317518510.1016/j.jas.2014.01.010 </plain></SENT>
</text></ref><ref id="ref-25"><text><SENT sid="299" pm="."><plain>McConville, Clauser &amp; Churchill (1980)McConvilleJClauserCChurchillTAnthropometric relationships of body and body segment moments of inertia1980Yellow SpringsAnthropology Research Project Inc </plain></SENT>
</text></ref><ref id="ref-26"><text><SENT sid="300" pm="."><plain>Mungiole &amp; Martin (1990)MungioleMMartinPEEstimating segment inertial properties: comparison of magnetic resonance imaging with existing methodsJournal of Biomechanics1990231039104610.1016/0021-9290(90)90319-X2229087 </plain></SENT>
</text></ref><ref id="ref-27"><text><SENT sid="301" pm="."><plain>Pearsall, Reid &amp; Livingston (1996)PearsallDJReidJGLivingstonLASegmental inertial parameters of the human trunk as determined from computed tomographyAnnals of Biomedical Engineering19962419821010.1007/BF026673498678352 </plain></SENT>
</text></ref><ref id="ref-28"><text><SENT sid="302" pm="."><plain>Pearsall, Reid &amp; Ross (1994)PearsallDReidJRossRInertial properties of the human trunk of males determined from magnetic resonance imagingAnnals of Biomedical Engineering19942269270610.1007/BF023682947872577 </plain></SENT>
</text></ref><ref id="ref-29"><text><SENT sid="303" pm="."><plain>Seitz et al. (2006)SeitzSMCurlessBDiebelJScharsteinDSzeliskiRA Comparison and Evaluation of Multi-View Stereo Reconstruction Algorithms2006 IEEE computer society conference on computer vision and pattern recognitionvol. </plain></SENT>
<SENT sid="304" pm="."><plain>12006PiscatawayIEEE519528 </plain></SENT>
</text></ref><ref id="ref-30"><text><SENT sid="305" pm="."><plain>Sellers &amp; Hirasaki (2014)SellersWIHirasakiEMarkerless 3D motion capture for animal locomotion studiesBiology Open2014365666810.1242/bio.2014808624972869 </plain></SENT>
</text></ref><ref id="ref-31"><text><SENT sid="306" pm="."><plain>Sheets, Corazza &amp; Andriacchi (2010)SheetsALCorazzaSAndriacchiTPAn automated image-based method of 3D subject-specific body segment parameter estimation for kinetic analyses of rapid movementsJournal of Biomechanical Engineering201013201100410.1115/1.400015520524742 </plain></SENT>
</text></ref><ref id="ref-32"><text><SENT sid="307" pm="."><plain>Siri (1961)SiriWEBroÅ¾ekJHenschelABody composition from fluid spaces and density: analysis of methodsTechniques for measuring body composition1961Washington D.C.National Academy of Sciences, National Research Council223244 </plain></SENT>
</text></ref><ref id="ref-33"><text><SENT sid="308" pm="."><plain>Spitzer et al. (1996)SpitzerVAckermanMJScherzingerALWhitlockDThe visible human male: a technical reportJournal of the American Medical Informatics Association1996311813010.1136/jamia.1996.962362808653448 </plain></SENT>
</text></ref><ref id="ref-34"><text><SENT sid="309" pm="."><plain>Straub &amp; Kerlin (2014)StraubJKerlinSDevelopment of a large, low-cost, instant 3d scannerTechnologies20142769510.3390/technologies2020076 </plain></SENT>
</text></ref><ref id="ref-35"><text><SENT sid="310" pm="."><plain>Triggs et al. (2000)TriggsBMcLauchlanPHartleyRIFitzgibbonAWBundle adjustmentâa modern synthesisLecture Notes in Computer Science20001883298372 </plain></SENT>
</text></ref><ref id="ref-36"><text><SENT sid="311" pm="."><plain>Weinbach (1938)WeinbachAContour maps, center of gravity, moment of inertia and surface area of the human bodyHuman Biology193810356371 </plain></SENT>
</text></ref><ref id="ref-37"><text><SENT sid="312" pm="."><plain>Winter (1979)WinterDABiomechanics of human movement1979New YorkWiley and Sons </plain></SENT>
</text></ref><ref id="ref-38"><text><SENT sid="313" pm="."><plain>Yeadon (1990)YeadonMThe simulation of aerial movementâII. </plain></SENT>
<SENT sid="314" pm="."><plain>A mathematical inertia model of the human bodyJournal of Biomechanics199023677410.1016/0021-9290(90)90370-I2307693 </plain></SENT>
</text></ref><ref id="ref-39"><text><SENT sid="315" pm="."><plain>Yokoi et al. (1998)YokoiTTakahashiAOkadaHOhyamaKBMuraokaMIs the selection of body segment interia parameters critical to the results of the kinematic and kinetic analysis of human movement?Anthropological Science199810637138310.1537/ase.106.371 </plain></SENT>
</text></ref><ref id="ref-40"><text><SENT sid="316" pm="."><plain>Zatsiorsky (2002)ZatsiorskyVMKinetics of human motion2002ChampaignHuman Kinetics </plain></SENT>
</text></ref></ref-list></SecTag></back></article>
